import requests
import os
import argparse
import json
import pandas as pd
from datetime import datetime
from tqdm import tqdm
from bs4 import BeautifulSoup
from openai import OpenAI
# importing necessary functions from dotenv library
from dotenv import load_dotenv, dotenv_values 
# loading variables from .env file
load_dotenv()


def create_gpt_4o_mini_prompt(original_claim, original_claim_label, our_sources, article):
    # Definitions: 
    # https://aclanthology.org/W14-2508.pdf
    # https://en.wikipedia.org/wiki/Misinformation
    string = f"""
    You are a fact-checking annotator trained to extract and categorize information from the given "Article" based on the "Original Sources", "Original Claim" and "Original Claim Label".
    Your task is to extract "Misinformation Sources" and "Fact-Checking Evidence" from the given "Article" based on the "Original Sources", "Original Claim" and "Original Claim Label".

    A fact-checking annotator is a role that helps to assign a truth value to a claim made in a particular context.

    Consider the following in your evaluation:
    
    Definitions:
    - Politifact Labels:
        - True: The statement is accurate and there’s nothing significant missing.
        - Mostly True: The statement is accurate but needs clarification or additional information.
        - Half True: The statement is partially accurate but leaves out important details or takes things out of context.
        - Barely True: The statement contains an element of truth but ignores critical facts that would give a different impression.
        - False: The statement is not accurate.
        - Pants on Fire / Pants-on-Fire: The statement is not accurate and makes a ridiculous claim.
    - Misinformation Sources: 
        - Information that is incorrect or misleading but is typically spread without malicious intent. 
        - This can include errors, misinterpretations, or contextually misleading statements that can be amplified or misunderstood when shared.
        - Along with each misleading statement, you must specify the source of origin, who made the statement, where it originated, and its description.
    - Fact-Checking Evidence: 
        - Verified information from reliable sources that is used to assess the veracity of a claim suspected of being misinformation. 
        - This can include statements from experts, official documentation, government officials, or trustworthy organizations that clarify misunderstandings or provide factual context to refute misleading claims.
        - Along with each verified information, you must include any supporting information or transitioning information that support this information.
    - Rating Sentence:
        - A sentence that indicate the final evaluation of the claim's accuracy based on "Politifact Labels".
    - Transition Sentence:
        - A sentence that introduces the "Fact-Checking Evidence" section's topic or argument, contain logical connectors or indicate a shift in focus, tone, or evidence from the "Misinformation Sources" section..
        - It is not the same as "Rating Sentence"
    - "Social Media Flag" Sentences:
        - Sentences that contains these sentences that indicates partnership with social media companies
            - "Social Media Flag" sentences examples:
                - "Read more about PolitiFact’s partnership with Meta"
                - "Read more about PolitiFact’s partnership with TikTok"
    - "Question" Sentences:
        - Sentences that is structured as a question and clearly indicates a transition between "Misinformation Sources" and "Fact-Checking Evidence".
            - "Question" sentences examples:
                - Is Hochul right? Have 732,000 jobs have been created since she became governor in late summer 2021?
    - "But" Sentences:
        - Sentences that starts with "But" and clearly indicates a transition between "Misinformation Sources" and "Fact-Checking Evidence".
            - "But" sentences examples:
                - But there’s no record Trump, the president-elect, ever said those words. These viral videos use old footage with what appears to be fake audio generated by artificial intelligence.
                - But these social media posts are wrong. Haley was born in South Carolina and meets the U.S. Constitution’s requirements to run for president.
    - "Action" Sentences:
        - Sentences that indicates an action and a transition between "Misinformation Sources" and "Fact-Checking Evidence". It is not the same as "Rating Sentence".
            - "Action" sentences examples:
                - For this fact-check, we examined only Biden’s comment about wages and inflation. 
                - We decided to look into how the university system has been funded in recent years.
    - "Reasoning" Sentences:
        - Sentences that does not have clear transition indicators, but are indicated as a "Transition Sentence" when the whole article is considered.
            - "Reasoning" sentences examples:
                - PolitiFact New Jersey found Doherty is mostly right: when a student is enrolled in the federally supported lunch program, they are designated as at risk. 

    Please remember these definitions.
    
    There are two tasks that you will need to do.
    
    Preprocessing Task:
    - Read the whole article
    - Identify the "Transition Sentence" from "Misinformation Sources" to "Fact-Checking Evidence" that are similar to "Social Media Flag" Sentences, "But" Sentences, "Question" Sentences, "Action" Sentences and "Reasoning" Sentences.
    - Use the "Transition Sentence" to split the article into "Misinformation Sources Chunk" and "Fact-Checking Evidence Chunk" without any modifications.
    - Retrieve the sentences from "Misinformation Sources Chunk" that are originated from "Original Sources" without any modifications as "Misinformation Sources".
        - Additional sentences that describe the main sentences must be included as well.
    - Retrieve the sentences from "Fact-Checking Evidence Chunk" that are originated from "Original Sources" without any modifications as "Fact-Checking Evidence".
        - Additional sentences that describe the main sentences must be included as well.
        
    Cleanup Task:
    - Do not include the "Transition Sentence" and "Rating Sentence" as an output for "Misinformation Sources" and "Fact-Checking Evidence".
    - The sentences in "Misinformation Sources" and "Fact-Checking Evidence" must be sorted based on the sentence ordering in the article.
    - Combine the sentences in "Misinformation Sources" based on the "Article" and "Original Sources".
    - Combine the sentences in "Fact-Checking Evidence" based on the "Article" and "Original Sources".
    - Return the "Misinformation Sources", "Fact-Checking Evidence", "Transition Sentence" and "Explanation" in JSON.

    Please perform the task as stated.
    
    Important rules:
    - You must consider all sentences in the article.
    - You are not allowed to rephrase or modify any texts from the article.
    - There cannot be two identical texts in both "Misinformation Sources" and "Fact-Checking Evidence".
    - "Misinformation Sources" sentences must include the person or source that mentioned the sentence.
    - Sentences that contain "Politifact Labels" regardless of its form must be excluded from the output.
    - "Transition Sentence" must not be included in "Misinformation Sources" and "Fact-Checking Evidence".
    - "Rating Sentence" must not be the same as "Transition Sentence".

    Please follow the rules strictly.
    
    "Original Claim": 
    {original_claim}
    
    "Original Claim Label": 
    {original_claim_label}
    
    "Original Sources":
    {our_sources}

    "Article":
    {article}
    """
    return string

def prepare_misinformation_sources_and_fact_checking_data(args):
    SAVE_PATH = args.save_path
    POLITIFACT_EXTRACTED_ARTICLES_FOLDER = args.politifact_extracted_articles
    metadata = os.path.join(POLITIFACT_EXTRACTED_ARTICLES_FOLDER, 'politifact_extracted_articles.json')

    if not os.path.exists(SAVE_PATH):
        os.makedirs(SAVE_PATH)

    # Save current args to output
    with open(f"{SAVE_PATH}/argsparse_config.json", 'w') as file:
        json.dump(vars(args), file)
        file.close()
        
    client = OpenAI(api_key=os.getenv("OPENAI_KEY"))

    df = pd.read_json(metadata).sort_values(by='year', ascending=False)
    df = df[~df['label'].isin(['no-flip', 'half-flip', 'full-flop'])] # These are the extra PolitiFact Labels that we will ignore.

    prompts = []
    for row in tqdm(df.iloc()):
        article_html_file_location = row['main_article_html_path']
        our_sources_file_location = row['our_sources_html_path']
        original_claim_html = row['claim']
        original_claim = original_claim_html.strip()
        original_claim_owner = row['claim_owner']
        original_claim = f"{original_claim_owner} said that {original_claim}"
        original_claim_label = row['label']
        article = None
        with open(article_html_file_location, 'r') as file:
            html = file.read()
            article_soup = BeautifulSoup(html,"lxml")
            article = article_soup
            file.close()
        
        our_sources = None
        with open(our_sources_file_location, 'r') as file:
            html = file.read()
            our_sources_soup = BeautifulSoup(html,"lxml")
            our_sources = our_sources_soup
            file.close()
        assert article is not None and our_sources is not None
        prompt = create_gpt_4o_mini_prompt(original_claim, original_claim_label, our_sources, article)
        prompts.append({'url': row['url'], 'prompt': prompt})
    
    # Prepare Batch API
    output_file_names = []
    gap = 100
    counter = 0
    for index in range(0, len(prompts), gap):
        current_prompts = prompts[index : index + gap]
        request_file_path = os.path.join(SAVE_PATH, f'requests_{index}.jsonl')
        with open(request_file_path, 'w') as outfile:
            for index, prompt_data in enumerate(current_prompts):
                counter = counter + 1
                print(prompt_data['url'])
                data = {
                    "custom_id": prompt_data['url'],
                    "method": "POST",
                    "url": "/v1/chat/completions",
                    "body": {
                        "model": "gpt-4o-mini",
                        "messages": [
                            {"role": "user", "content": prompt_data['prompt']},
                        ],
                    }
                }
                json.dump(data, outfile)
                outfile.write('\n')
            outfile.close()
        output_file_names.append(request_file_path)

    final_details = []
    for file_path in tqdm(output_file_names):
        batch_input_file = client.files.create(
            file=open(file_path, "rb"),
            purpose="batch"
        )
        batch_input_file_id = batch_input_file.id
        output = client.batches.create(
            input_file_id=batch_input_file_id,
            endpoint="/v1/chat/completions",
            completion_window="24h",
            metadata={
            "description": f"Politifact Misinformation Sources & Fact-Checking Extraction with file {file_path}"
            }
        )
        final_details.append({
            "batch_id": output.id,
            "original_file": file_path
        })

    with open(os.path.join(SAVE_PATH, 'output.json'), 'w') as outfile:
        for detail in tqdm(final_details):
            json.dump(detail, outfile)
            outfile.write('\n')
        outfile.close()

if __name__ == '__main__':
    timestamp = datetime.now().strftime("%Y-%m-%d--%H-%M-%S")
    parser = argparse.ArgumentParser(description='Politifact Link Extractor. Extract Personalities and Article Links from targeted URLs')
    parser.add_argument('--save_path', type=str, default=os.path.join('script_outputs', 'politifact-gpt-4o-mini-extractor-requests'), help='Script output location')
    parser.add_argument('--politifact_extracted_articles', type=str, default=os.path.join('script_outputs', 'politifact-extracted-articles'), help='politifact_articles_links.csv location')

    args = parser.parse_args()
    prepare_misinformation_sources_and_fact_checking_data(args)